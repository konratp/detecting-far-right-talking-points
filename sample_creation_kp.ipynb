{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5dcc49c",
   "metadata": {},
   "source": [
    "# Ideas behind Sampling:\n",
    "\n",
    "In this notebook, we try three different approaches to creating a training set with which we will train our deep learning model. First, we downsample the data frame to counter an imbalance that can be found in the data. Second, we use an upsampling strategy, essentially creating copies of observations that are in the minority class. Finally, we artificially create a new training set by splitting each observation into sentences and rearranging observations at shorter lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d88d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3427846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data frame\n",
    "path = f\"{os.getcwd()}\"\n",
    "data = pd.read_csv(f\"{path}/final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8abbaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test sets\n",
    "X = data[['contribution_text']]\n",
    "y = data['far_right']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = X_train.join(y_train)\n",
    "test = X_test.join(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8b03a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename train data as data and run the following code\n",
    "data = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "51bc0964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save test set to csv file\n",
    "#test.to_csv(f\"{path}/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "25e9e4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/konratpekkip/code/konratp/Detecting-Far-Right-Talking-Points'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5617b041",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "02a6ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82341,17518\n"
     ]
    }
   ],
   "source": [
    "#split data into two groups based on y variable\n",
    "category_0 = data[data['far_right'] == 0]\n",
    "category_1 = data[data['far_right'] == 1]\n",
    "print(f\"{len(category_0)},{len(category_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7433ad37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    35036\n",
       "1    17518\n",
       "Name: far_right, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take sample of y=0 in order to downsample and create better balance between features\n",
    "cat_0_sample = category_0.sample(len(category_1)*2)\n",
    "d_sample = pd.concat([cat_0_sample, category_1], axis=0)\n",
    "d_sample['far_right'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a8c86334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "#d_sample.to_csv(f\"{path}/downsample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9163aaa5",
   "metadata": {},
   "source": [
    "## Upsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "656f6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_1_usample = pd.concat([category_1, category_1], axis = 0)\n",
    "u_sample_1 = pd.concat([cat_1_usample, cat_1_usample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "be030d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_sample = pd.concat([u_sample_1, category_0], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7a468091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "#u_sample.to_csv(f\"{path}/upsample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d60c9a",
   "metadata": {},
   "source": [
    "## Synthetic Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b0339",
   "metadata": {},
   "source": [
    "In this step, we split each contribution into individual sentences and try to create new observations from these sentences that are shorter in length, but thus more plentiful than the previous subsets of the data for which far_right = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "77ccb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of all sentences in the data set\n",
    "list_of_sentences = []\n",
    "\n",
    "for i in category_1.contribution_text:\n",
    "    k = nltk.tokenize.sent_tokenize(i)\n",
    "    for j in k:\n",
    "        list_of_sentences.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3c614095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185428"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many sentences are present in the data set\n",
    "len(list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fa602f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.58499828747574"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check average number of sentences per contribution\n",
    "len(list_of_sentences) / len(category_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a65607fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1472.9630665601096"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check average number of characters per contribution\n",
    "avg_length = np.mean(category_1.contribution_text.map(len))\n",
    "avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8ec9fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize list of sentences\n",
    "random_list_of_sentences = random.sample(list_of_sentences, len(list_of_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3b9676af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary that reassembles sentences and synthesizes new observations -- triple number of obs where far_right = 1\n",
    "\n",
    "synth_dict = {}\n",
    "list_of_charcounts = []\n",
    "\n",
    "for i in range(len(category_1) * 3):\n",
    "    sample = []\n",
    "    char_count = 0\n",
    "    for j in range(10):\n",
    "        rand_idx = random.randint(0, len(category_1)-1)\n",
    "        sentence = random_list_of_sentences[rand_idx]\n",
    "        sample.append(sentence)\n",
    "        char_count += len(sentence)\n",
    "    if char_count < avg_length:\n",
    "        rand_idx = random.randint(0, len(category_1)-1)\n",
    "        sample.append(random_list_of_sentences[rand_idx])\n",
    "        char_count += len(random_list_of_sentences[rand_idx])\n",
    "    list_of_charcounts.append(char_count)\n",
    "    synth_dict[i] = [sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1ac1770d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2887696464588316"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check difference in average number of characters per observation\n",
    "avg_length - np.mean(list_of_charcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "8a30da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe based on the synth sample dictionary\n",
    "s_sample_1 = pd.DataFrame.from_dict(synth_dict).T\n",
    "s_sample_1['far_right'] = 1\n",
    "s_sample_1 = s_sample_1.rename(columns = {0: 'contribution_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "365f580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shorten cat_0 data frame to be able to concat later on\n",
    "cat_0_short = category_0[['contribution_text', 'far_right']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0900ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate newly synthesized rows with previously existing rows where far_right = 0, shuffle and reset index\n",
    "s_sample = pd.concat([cat_0_short, s_sample_1], axis = 0)\n",
    "s_sample = s_sample.sample(frac = 1)\n",
    "s_sample.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "093c348f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    82341\n",
       "1    52554\n",
       "Name: far_right, dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check value counts of far_right column\n",
    "s_sample['far_right'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f586d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to csv\n",
    "#s_sample.to_csv(f\"{path}/synthsample.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
