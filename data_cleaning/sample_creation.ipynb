{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2b1694",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a46546",
   "metadata": {},
   "source": [
    "In order to accurately train a deep learning model, it is often not enough to simply train it on a complete dataset and expect it to be able to make accurate predictions. Rather, it is important to account for potential biases in the underlying data, and train a model on a more balanced dataset.\n",
    "\n",
    "Because most Members of the European Parliament (MEP) are members of conservative, left-wing, environmentalist or liberal parties, it is only natural that most speeches in the European Parliament are not given by right-wing populists, but rather politicians from a variety of ideological backgrounds. Since our goal is to train a deep learning classifier to accurately predict whether a given speech was given by a right-wing populist MEP or not, training it on the original, unbalanced data would likely not yield satisfying results. Since only about 15% of the speeches in our dataset were given by right-wing populists, a model trained on the unbalanced, original dataset could simply just predict every speech not to be given by a right-wing populist MEP, and still be right in 85% of cases. \n",
    "\n",
    "To avoid such an outcome, we instead want to train the model on balanced data. In this notebook, we transform our dataset to become more balanced. We use three different approaches for this: downsampling, upsampling, and synthetic sampling. To reproduce the results of our project, we recommend running the code below to retrace our steps and understand the conceptual differences between the three balancing approaches before moving on to the notebooks training our BERT classifier and performing topic modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d680a",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "In the first few cells of code, we load required packages as well as the dataset resulting from cleaning and preprocessing steps taken in the [`data_cleaning` notebook](data_cleaning_preprocessing.ipynb). Please make sure to read in the cleaned and preprocessed data here. After reading in the data frame, we split the data into a test and train set, as we do **not** want to balance the test data, only the training set. If you choose to do so, you can save the test set as a `.csv` file by uncommenting the relevant line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ba980c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503ae349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data frame\n",
    "path = f\"{os.getcwd()}\"\n",
    "data = pd.read_csv(f\"{path}/final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ee317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test sets\n",
    "X = data[['contribution_text']]\n",
    "y = data['far_right']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#join X_train and y_train\n",
    "data = X_train.join(y_train)\n",
    "\n",
    "#join X_test and y_test\n",
    "test = X_test.join(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03bc8a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the following line to save test set to csv file\n",
    "#test.to_csv(f\"{path}/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e43b8",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "\n",
    "Downsampling is an approach to balancing a dataset that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c7eaeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of speeches given by non-far-right MEP is 82346, while the number of speeches given by far-right MEP is 17513.\n"
     ]
    }
   ],
   "source": [
    "#split data into two groups based on y variable\n",
    "category_0 = data[data['far_right'] == 0]\n",
    "category_1 = data[data['far_right'] == 1]\n",
    "print(f\"The number of speeches given by non-far-right MEP is {len(category_0)}, while the number of speeches given by far-right MEP is {len(category_1)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23be974c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    35026\n",
       "1    17513\n",
       "Name: far_right, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce number of speeches where far_right = 0 -- create a dataset with 1/3 far_right = 1, 2/3 far_right = 0\n",
    "cat_0_sample = category_0.sample(len(category_1)*2)\n",
    "d_sample = pd.concat([cat_0_sample, category_1], axis=0)\n",
    "d_sample['far_right'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d64316c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the following line to save downsampled data to csv file\n",
    "#d_sample.to_csv(f\"{path}/downsample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b650ff4",
   "metadata": {},
   "source": [
    "## Upsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "88a9a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_1_usample = pd.concat([category_1, category_1], axis = 0)\n",
    "u_sample_1 = pd.concat([cat_1_usample, cat_1_usample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "553a39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_sample = pd.concat([u_sample_1, category_0], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "42ca3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "#u_sample.to_csv(f\"{path}/upsample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e07c27",
   "metadata": {},
   "source": [
    "## Synthetic Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0322aac6",
   "metadata": {},
   "source": [
    "In this step, we split each contribution into individual sentences and try to create new observations from these sentences that are shorter in length, but thus more plentiful than the previous subsets of the data for which far_right = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "daecd134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of all sentences in the data set\n",
    "list_of_sentences = []\n",
    "\n",
    "for i in category_1.contribution_text:\n",
    "    k = nltk.tokenize.sent_tokenize(i)\n",
    "    for j in k:\n",
    "        list_of_sentences.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5e7505b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185428"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many sentences are present in the data set\n",
    "len(list_of_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cc3bc935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.58499828747574"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check average number of sentences per contribution\n",
    "len(list_of_sentences) / len(category_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "00179fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1472.9630665601096"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check average number of characters per contribution\n",
    "avg_length = np.mean(category_1.contribution_text.map(len))\n",
    "avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ffc8d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomize list of sentences\n",
    "random_list_of_sentences = random.sample(list_of_sentences, len(list_of_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3f9879ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary that reassembles sentences and synthesizes new observations -- triple number of obs where far_right = 1\n",
    "\n",
    "synth_dict = {}\n",
    "list_of_charcounts = []\n",
    "\n",
    "for i in range(len(category_1) * 3):\n",
    "    sample = []\n",
    "    char_count = 0\n",
    "    for j in range(10):\n",
    "        rand_idx = random.randint(0, len(category_1)-1)\n",
    "        sentence = random_list_of_sentences[rand_idx]\n",
    "        sample.append(sentence)\n",
    "        char_count += len(sentence)\n",
    "    if char_count < avg_length:\n",
    "        rand_idx = random.randint(0, len(category_1)-1)\n",
    "        sample.append(random_list_of_sentences[rand_idx])\n",
    "        char_count += len(random_list_of_sentences[rand_idx])\n",
    "    list_of_charcounts.append(char_count)\n",
    "    synth_dict[i] = [sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a0eea6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2887696464588316"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check difference in average number of characters per observation\n",
    "avg_length - np.mean(list_of_charcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ca9ba56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe based on the synth sample dictionary\n",
    "s_sample_1 = pd.DataFrame.from_dict(synth_dict).T\n",
    "s_sample_1['far_right'] = 1\n",
    "s_sample_1 = s_sample_1.rename(columns = {0: 'contribution_text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bb000047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shorten cat_0 data frame to be able to concat later on\n",
    "cat_0_short = category_0[['contribution_text', 'far_right']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "171659b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate newly synthesized rows with previously existing rows where far_right = 0, shuffle and reset index\n",
    "s_sample = pd.concat([cat_0_short, s_sample_1], axis = 0)\n",
    "s_sample = s_sample.sample(frac = 1)\n",
    "s_sample.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "153d952d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    82341\n",
       "1    52554\n",
       "Name: far_right, dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check value counts of far_right column\n",
    "s_sample['far_right'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "44df49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to csv\n",
    "#s_sample.to_csv(f\"{path}/synthsample.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
